Introduction

Making near term forecasts for the future state of ecological systems allows end users of a forecast to have the latest predictions from all available data and allows for a quick turn around in forecast evaluation [@dietze2018]. Despite these benefits, and a growing amount of research into ecological forecasting [@petchey2015, @luo2011, @dietze2017], few systems are in place that make regularly updating near term ecological forecasts. -@white2018 developed a system to forecast rodent abundance at a single long term study site in Southern Arizona that required expertise in modelling, data analysis, continuous integration, and local knowledge of the site and rodent community itself. While -@white2018 is an excellent example of a fully automated and continuously updated ecological forecast system (also see @doren2018), the approach does not  match all use cases in ecology. Other automated forecast systems will require models other than population time series models and need to be at scales larger than a single long term study site. More forecasting systems should be built for different systems and scales to explore the many potential use cases and further develop best practices. From these first implementations standardized methods, data formats, and software packages can be developed to help ecological forecasting become as straightforward as other analysis performed in ecology. 

Plant phenology has a long history of research in fields such as ecology, forestry, and climate science and is intrinsic in many ecological systems [@chuine2017]. The timing of events such as flowering and leaf out affect small scale community interactions [@ogilvie2017b] as well as global scale climate [@richardson2012]. Numerous phenology models currently exist to fit the needs of different ecosystems and life history strategies [@chuine2013]. The basic structure of most models can be used with data from either direct observations or ground and satellite based remote sensing. With a well established history and commonality across data sources and modelling frameworks, plant phenology is well suited for near term forecasting. Additionally, there is potentially high demand for near term phenology forecasts. Flower blooms and autumn colors drive tourism in many regions, the exact timing of which vary year to year. Researchers and managers planning activities involving short events, such as flowering or leaf out, would also benefit from a phenology forecast. With a long history of study and several potential use cases, a near term plant phenology forecast would be an ideal example of the potential uses of ecological forecasting. Developing such a system introduces several challenges.

Plant phenology is primarily driven by weather. In temperate latitudes seasonal temperature changes are the primary phenology driver [@chuine2017, @basler2017]. Thus, forecasting plant phenology requires forecasts of future climate conditions. Seasonal climate forecasts, those beyond just a few weeks, have moved from research to operational status in the past two decades [@weisheimer2014]. Currently the European Centre for Medium-Range Weather Forecasts and National Oceanic and Atmospheric Administration (NOAA) produce seasonal forecasts from 7-9 months into the future.  These forecasts are delivered in gridded datasets representing the entire earth's surface, and are updated several times daily using the latest initial conditions. This introduces an infrastructure challenge in converting the forecasts to scales relevant to ecology in an automated way. Phenology is especially sensitive to local climate conditions and large grained gridded datasets are not adequate for phenology models [@cook2010]. Downscaling methods are available to convert climate forecasts to a spatial scale more suitable for ecological studies [@fowler2007, @abatz2012, @stoner2012], but implementing them in a robust and automated way requires both advanced skills in scientific computing [@wilson2014] and the use of large scale computational resources. Software which can efficiently operate on datasets too large to fit into computer memory will need to be utilized. To date no ecological study has utilized these 7-9 month climate forecasts, likely because of these implementation and computational challenges. 

To make meaningful species level forecasts of plant phenology, as many species as possible should be included, especially common species. The forecast should also be over the entire potential range of each respective species. Models fit with data from a single site will not be adequate for these large scale forecasts as the phenological requirements can vary spatially [@chuine2017, @taylor2018b], so observations from as much of a species range as possible will be needed to fit large scale models. Currently the only large scale source of phenology data in the U.S.A. is the USA National Phenology Network (USA-NPN). Over the past decade volunteers have contributed over 10 million observations representing the phenology of more than 1000 species. The current state of phenology modelling is such that a single parameterized model, or an ensemble of models, is used for each species and phenophase. Thus, making forecasts for the timing of leaf out, flowering, and leaf senescence for 100 species requires 300 separate models. Each model must be individually parameterized, evaluated, and stored for future use. Any updates in available data or introduction of new modeling techniques will require this process to be repeated in a timely fashion. Parameterization of process based phenology models is commonly done using global optimization algorithms, which can be computationally expensive. Finally, making phenology predictions using large scale climate forecasts requires applying the model to a time series of climatic conditions for each climatic grid cell. Considering the continental U.S.A., with a spatial resolution of 4km, this becomes approximately 480,000 individual time series with which to apply each phenology model.

Forecasts are intended to inform end users, thus an easily understandable and consistently up-to-date presentation of phenology forecasts will be needed to fulfill this requirement. Forecasts should be presented such that the predicted date of a phenological event for every species and location is easily discernible. Presentations of forecasts should be updated as often as possible to reflect the latest knowledge about the climate and other potential drivers. A website to present phenology forecasts is an obvious choice for medium, but linking that in the automation pipeline with derived forecasts will again require advanced programming skills to work with web based methods.

This paper describes the methods for implementing an automated near term phenology forecast system. This system issues continental scale forecasts for specific plant species with lead times of up to six months. It is updated several times a week with the latest climate information and makes forecasts for different phenophases depending on the current season. All forecasts are automatically updated on the website http://phenology.naturecast.org. The main sections of the paper describe details of: 1) preparing climate forecast data for use in ecological modelling, 2) building the plant phenology models and applying them to climate forecasts, and 3) the presentation of the forecasts. In the discussion we outline major design decisions, lessons learned from implementing this system, as well as future steps to improve aspects of the system.










Figure 1: Flowchart of initial model building and automated pipeline steps. Letters indicate the associate steps discussed in the main text. 











Challenges

Climate Forecast Processing
The Climate Forecast System Version 2 (CFSv2) is a coupled atmosphere-ocean-land global circulation model maintained by the National Oceanic and Atmospheric Administration (NOAA)[@saha2014]. The model tracks over 1000 global state variables of varying resolution and forecast length, such as ocean temperature and heights of pressure bands. Here we use the 2-meter temperature variable, which has a 6-hour timestep and a spatial resolution of 0.25 degrees latitude/longitude. The forecast is updated every 6 hours with the latest initial conditions and projected out 9 months. 

The CFSv2 also has a reanalysis available. A climate reanalysis is a run of the full model over a prior time period with constant assimilation of known conditions. In practice this allows for analysis of state variables which are not able to be measured (such as the 500mb height over the arctic in winter). Here it allows us to build a downscaling model using the CFSv2 model’s best estimate of past conditions of land surface temperature. These past conditions are regressed against finer grained “known” conditions from a different gridded dataset on a per pixel basis. We used the 2-m temperature output from the reanalysis from 1995-2015 as well as 4km daily mean temperature from the PRISM dataset [@prismdata] to build a downscaling model using asynchronous regression (Figure 1, E-G). The model and theory are described in -@stoner2013 and references therein. The CFSv2 data is first interpolated from the original 0.25 degree grid to a 4km grid using distance weighted sampling, then the following method is applied to each 4km pixel and calendar month.

Collect all daily mean temperature observations from 21 years of data from both the CFSv2 reanalysis and the PRISM dataset. This provides 588 - 641 points representing daily temperature for a single pixel and calendar month. 
In addition to the data from each calendar month, also include data for the 14 days prior and 14 days following the calendar month, adding an addition 588 data points (21*(14+14)). This helps account for future novel conditions.
Order each dataset by their rank, such that the lowest value from the PRISM dataset is matched to the lowest value from the CFSv2 reanalysis.
Fit a linear regression model.

The two parameters from the regression model are saved in a netCFD file which can later be referenced by location and calendar month (Figure 1, H). This downscaling model, at the scale of the continental U.S.A., is used to downscale the most recent CFSv2 forecasts to a 4km resolution during the automated steps. 


Climate Automated Forecast Steps
The most recent daily mean temperature grids from the PRISM dataset are downloaded from the PRISM servers (Figure 1, J). Due to the quick turn around of assimilating the most recent temperature observations from across the U.S.A., these are released daily in a provisional status and then updated at a future date to a more stable version. These observed data of the current season are saved in a single netCDF file, which is appended with the most recent days everytime the automated forecast is run. 

Uncertainty in climate forecasts is represented by ensembles, where a model is run several times with slightly different initial conditions. Due to the chaotic nature of the atmosphere each of these runs produces a slightly different forecast of the desired state variable. From these ensembles probabilistic, as opposed to deterministic, forecasts can be made [@weisheimer2014]. To characterize climate uncertainty here, we use the 5 most recent CFSv2 forecasts (Figure 1, I). Each of these ensemble members represent a 9 month forecast and differ in the start time of the initial conditions. For example, if starting an automated phenology forecast at 0900 on Sep. 1, the forecasts from 06:00 Sep 1., 00:00 Sep. 1, 18:00 Aug. 31, 12:00 Aug. 31, and 06:00 Aug. 31 will be downloaded. If any forecast is not available than an earlier one is queried until 5 have been successfully downloaded and processed. Each ensemble member is first aggregated to a daily mean and subset to a standard time length. Each member is then interpolated to a 4km grid using distance weighted sampling, and then individual pixel values for each month are adjusted using the downscale model described above (Figure 1, K). 

Each forecast ensemble member is merged with the observed data from the PRISM dataset (Figure 1, L). This creates 5 daily mean temperature time series for the continental U.S.A., which have the same observed temperature up to the day the forecast is compiled, and differ in their forecasted temperature due to the different initial conditions. Each phenology model is used to make a prediction on each of the 5 members, creating a phenology forecast ensemble which accounts for some of the uncertainty in climate. 

Phenology Data Processing
The USA-NPN protocol uses status-based monitoring, where via a phone app or web based interface observers answer 'yes,' 'no,' or 'unsure' when asked if an individual plant has a specific phenophase present [@npncitation, @denny2014]. Phenophases refer to specific phases in the annual cycle of a plant, such as the presence of emerging leaves, flowers, fruit, or senescing leaves. USA-NPN provides several products summarizing the data in different ways. We downloaded the “Individual Phenometrics” product for all species with data between 2009 and 2017. We only kept “yes” observations where the individual plant also had a “no” observation within the prior 30 days. We dropped any records marked with a conflict, which can happen by disagreement in the phenophase status at group sites, where multiple observers can make observations on the same plants. Another potential conflict is greater than one series of “yes” observations for a phenophase on a single plant in a 12 month period. After filtering we kept species and phenophase combinations with at least 30 total observations for model building (Figure 1, B). For each USA-NPN site location  we extracted the daily mean temperature time series from 2009-2017 of the PRISM 4km dataset [@prismdata].
For each species and phenophase we built a suite of four models using daily mean temperature as the sole driver (Figure 1, C) [@taylor2018]. Three of the models were process based growing degree day models (Thermal Time, Uniforc, and Alternating), with one of those three being a 2-phase model (ie. includes a chilling element as well as warming, the  Alternating model). The final model was a linear model based off the mean spring temperature, where the start and length of spring are estimated parameters as well as the regression coefficients. 


Model
DOY Estimator
Forcing Equation
Ref.
Thermal Time


[@reaumur1735, @wang1960, @hunter1992]
Uniforc


[@chuine2000]
Alternating


[@cannell1983]
Linear



-

Instead of performing model selection we used a weighted ensemble of the four models for each species and phenophase. Ensembles generally improve prediction over any single model, and in a phenology context allow more accurate predictions to be made without knowing a priori the specific physiological processes for each species [@basler2016, @yun2017, @dormann2018]. An ensemble phenology model also allows for model uncertainty to be included in the forecasts. Processed based phenology models do not have numerical solutions for variance, thus the weighted variance from the four sub model predictions are used to represent model uncertainty. The weights for each model within the ensemble were derived via stacking as described in -@dormann2018. The steps for calculating weights are as followed:
Subset the data into random training/testing sets.
Fit each core model on the training set.
Make predictions on the testing set.
Find the weights which minimize RMSE of the testing set.
Repeat 1-4 for 100 iterations.
Take the average weight for each model from all iterations as the final weight used in the ensemble. These will sum to 1.
Fit the core models a final time on the full dataset. Parameters derived from this final iterations will be used to make predictions, which are then used in the final weighted average.

The specifications for each fully fitted model are saved in a text based json file for later use (Figure 1, D). These text files are themselves saved in a git repository, along with a metadata file describing all models, hosted on the GitHub website. All code specific to building, saving, loading, and making predictions with phenology models has been built into a dedicated software package [@taylor2018a]. 

Phenology Automated steps
Each phenology model (representing a single species and phenophase) is used to make predictions on each of the 5 climate ensemble (Figure 1, M). Note each climate ensemble is a 3d matrix of latitude*longitude*time at daily timesteps and the pyPhenology package was designed to operate on it without further processing.  The output of the model is a 2d matrix (latitude*longitude) where each cell represents the predicted Julian day of the phenological event. Since the phenology models are themselves an ensemble of 4 models, for each climate ensemble the weighted mean of the 4 phenology models is used. The output of each model is cropped to the range of the respective species and then saved as a netCDF file (Figure 1, N). These saved forecasts are used to create the images used in the forecast presentation and are stored for later evaluation. 

Presentation (website)
The saved phenology forecasts, in netCFD files, are used to create static images representing 1) the predicted julian day and 2) the 95% confidence interval, in days, of the predicted julian day (Figure 1, O). After all the images are generated they are uploaded to a Google Cloud storage container along with a metadata file with the details of each image (Figure 1, P). The upload is done using the provided Google Cloud API implemented in python. A web interface was written using JavaScript which displays a forecast for a specific species, phenophase, and issue date according to a selection in a set of drop-down boxes (Figure 1, Q; Figure 2).
The website is hosted on the Google Cloud storage container and is independent from the rest of the infrastructure. It displays the most recent (and all prior) phenology forecasts. It is only updated if all prior steps of the automation pipeline were successfully run. In this way the presentation of the most recent forecasts are not affected by potential errors in the automated pipeline.







Figure 2. Screenshot of the forecast presentation website (http://phenology.naturecast.org) showing the February 8, 2018 forecast for the leaf out of Acer negundo. In the upper right is the interface for selecting different species, phenophases, or forecast issue dates via drop down menus.
Discussion
We created an automated plant phenology forecast system which makes forecasts for specific species and phenophases at a continental scale. Forecasts are updated twice weekly with the most recent climate forecasts, converted to static maps, and uploaded to a website for presentation. The website interface is interactive such that a user can view any available species, phenophase, or forecast issue date for the current season. The system was made entirely using open source software and data formats, and free publicly available data. The primary costs of the system after developer time are server costs from the model building processes.  While evaluation of forecast performance is outside the scope of this paper, we note that the majority of forecasts provide realistic phenology estimates across known latitudinal and elevational gradients, and forecast uncertainty decreases as spring progresses (Figure 3). 



Figure 3. Progression of the forecast for the flowering of Cercis canadensis from January 20, 2018 to March 22, 2018. 

Software Used
Throughout the codebase both R and python are used. These are the two primary programming languages used in many fields, and often only one is chosen for any particular project based on researcher preference. Here we leverage the strength of both inside a complex pipeline that includes automated processing of 10’s of gigabytes of data and visualization of results. In such a pipeline no single language will fit all requirements, thus we make use of the strengths of these two languages. Interoperability is facilitated by common data formats, allowing scripts written in one language to communicate results to the next step in the pipeline written in another language.  

The R language was used for the initial phenology data ingesting and formatting, and the final forecast visualizations. Phenology data provided by the USA-NPN needed to be filtered of unsuitable observations and transformed into a format for use in the pyPhenology modeling package. With the tidyverse suite of packages in R data manipulation is made clear and reproducible thru the use of function names derived from verbs, and the usage of a pipe operator which facilitates easy reading of code [@dplyr, @tidyr]. The grammar of graphics package in R (ggplot2, also part of the tidyverse) is currently one of the most feature complete and user friendly visualization packages [@ggplot2]. Here we use ggplot2 to build maps of the forecasted phenological events. The spatial domain of all species is the same (the continental U.S.A.), thus the same map creation routines can be used for every species while only changing the underlying forecast. The ggplot2 package also has built in functionality for spatial data, making the display of maps in the desired coordinate system straightforward. 

The python language was used for manipulating climatic datasets, and for building and applying phenology models. After downscaling, the final size of the the climate forecast ensemble is 10-40 gigabytes, depending on the time of year and the size of the timeseries. Servers capable of fully loading this dataset into memory are available, but a more efficient approach is to perform operations iteratively across the array. For example; by subsetting the dataset spatially and performing operations on only one chunk at a time. The python package xarray allows this type of operation with minimal effort thru tight integration with the dask package [@xarray, @dask]. This allows complex operations to be performed even on desktop systems with only several gigabytes of RAM, as long as the chunk size operated on is small enough. With this in place extending the pipeline to work on high performance computing will be straightforward as the chunks are operated on independently. Adding more data, either thru additional variables such as precipitation or increasing the spatial extent beyond the U.S.A, will also be simple since, regardless of the dataset size, only the chunk size needs to be adjusted to run on the system available. Methods such as this are not available out of the box in similar R packages, thus python was a natural choice for processing the large meteorological datasets in this pipeline. 

Using two programming languages here is facilitated by the use of open data formats for passing information between languages. We used  csv and netCDF files for this purpose. csv files are ubiquitous in science and becoming even more popular thru the use of tidy data formats as well as data analysis and management training [@wilson2014, @hampton2017].Because of this csv files can be read and written by all common programming languages. The netCDF file is a standard for data stored in large labelled arrays, such as a time series of daily temperature for gridded spatial data. The python xarray package was built to manipulate gridded data such as this and also seamlessly read and write netCDF files [@xarray]. netCDF files are predominantly used in meteorology and climatology, but could see an increased use in ecology as modeling moves toward the output of large gridded datasets. 

Other software packages used throughout the system were raster, prism, sp, lubridate, ncdf4, ….. And from the python language numpy, pandas, and mpi4py. All code described is available on a GitHub repository (https://github.com/sdtaylor/phenology_forecasts) and permanently archived on Zenodo [@].

Lessons learned

Partitions major chunks into independent packages. This makes maintenance easier as the package can have a specific API, have it’s own independent tests using continuous integration, and be under version control to track package changes. The latter point also allows the entire system to be put into a containerized environment, where software package versions throughout the pipeline remain unchanged. A containerized environment eliminates the possibility of package API updates introducing unexpected changes and breaking the pipeline [@boettiger2015]. Packaging should also make methods more generalized as opposed to having specific routines for a particular system, which makes it easier for others to implement the package for their own systems. In this system we split the phenology modelling component into an independent software package, pyPhenology. While pyPhenology was written with the intention of solving some of the data integration problems described throughout this paper, it is generalized such that it can be used for any research using process based phenology models.

Setup a system for saving, documenting, and updating parameterized models. Here we have built several hundred process based phenology models, all of which will be updated over time with new data and/or methods. We used the version control system git, hosted on GitHub, to hold the text based model specifications. While the git environment was originally designed for version control of software source code (indeed, a separate repository is used to house the source code behind the entire pipeline), it can also be leveraged for tracking data [@ram2013, @bryan2018, @yenni2019]. In this case the data are the numerous parameterized phenology models saved in text based json files. This setup has several benefits that allows for the tracking and usage of hundreds of models. Models are easily synchronized across systems, and versioned updating of models is done using git commit comments to document model history. Even if an older model is deleted in the primary repository, the git environment will archive it indefinitely for future reference. Managing many different models, including their different versions and provenance, will be likely be a ongoing challenge in ecological forecasting and this setup provides just one potential solution to address this [@white2018].

Use status checks and logging throughout the automated pipeline to easily identify problems. Throughout the system every major step logs the specific action being taken, a timestamp, and whether the step failed or succeeded. For example before applying the phenology models, the downscaled climate forecast files are checked for having the correct number of ensemble members, and that each member has the expected time series length. Either of these components can be deficient if, for example, the internet connection of the server was interrupted or if there was an error in the processing the CFSv2 forecasts. If both these conditions are not met then the automated pipeline is halted and the specific reason logged. With this information a problem can easily be identified and corrected with minimal downtime. 

Make the presentation of forecasts disconnected from the rest of the pipeline. In this automated pipeline the website is only updated if all other prior steps ran successfully. This setup allows for a failure in the forecasting pipeline to happen without affecting the presentation, such that a user on the website can always access the latest forecasts. The website is also hosted on a Google Cloud container, which provides a guarantee of uptime and is separate from the dedicated server the automated pipeline is run on. Thus even with a total backend server failure, the latest forecasts would still be available to users via the hosted website. 



Model improvement
More accurate and precise phenology models are the crux of a large amount of phenology research [@tang2016]. Here we chose an ensemble model, with four core models, as a first pass to implement the fully automated system. There are several areas where modelling can be improved. Inclusion of other core models with a chilling component may help budburst and flowering for temperate plants (currently only the Alternating model includes a chilling component). Models including precipitation would likely have greater accuracy for species in the U.S.A. Southwest. Calibrating several hundred different models to provide the best forecasts remains challenging. Observational data may not be available for the full range of a species, and other factors besides climate may be driving the phenology at local scales [@diez2012]. Models are currently optimized to minimize the error of an out of sample subset of the data, as is standard practice for process based phenology models [@chuine2017]. This does not account for spatial or temporal autocorrelation, and assumes there is no within species variation in phenological requirements. 

Currently only a single driver, daily mean temperature, is updated in the automated pipeline. Observations from the USA-NPN are available in near realtime after they are submitted by volunteers, thus there is opportunity for data assimilation of phenology observations. Making new forecasts with the latest information not only on the current state of the climate, but also on the current state of the plants themselves would likely be very informative [@luo2011, @dietze2017]. The process based models currently employed here are not designed for data assimilation though, indeed no process based plant phenology has been designed with data assimilation in mind [@chuine2013]. -@clark2014b built a bayesian hierarchical phenology model of budburst which incorporates the discrete observations of phenology data, and could likely serve as a starting point for a phenology forecasting model which incorporates data assimilation. The model from -@clark2014b also incorporates all stages of the bud development process into a continuous latent state, thus there is also potential for forecasting the current phenological state of plants, instead of just the transition dates as currently done in this forecast system. 

A hierarchical model which includes species level random effects may provide better performance for phenology forecasts. Many species, especially in northern latitudes, have similar phenology patterns [@basler2016]. In a hierarchical model species with relatively few observations could borrow strength from species with several thousand observations. This setup could also allow the ability for data assimilation as the seasons progress. For example if an observation is made indicating a phenological event has not happened yet, that could inform the next round of forecasts as to whether that species, region, and/or phenological event was late. 


Improved presentation
The current maps of predicted phenology show the large spatial trends across each species respective range, but they may be too coarse for actual decision making. The spatial resolution of the underlying forecast data, at 4 km, is fine enough to allow for zoomable and interactive maps which could increase usability. More informative ways to present and communicate the forecasts could also be developed. For example, the current website answers the question: “When will a specified species transition to a specified phenophase at a specified place?”. A more informative question might be: “Which species will be in bloom at a specified time and place?”. The data and methods are available to answer the latter question, with the largest hurdle being user interface design. 

Conclusions 

Using open source software, open data formats and data sources, and having training from workshops, we have implemented an automated phenology forecast system. Major challenges included the automated processing of large meteorological datasets, efficient application of hundreds of phenological models, and stable, consistently updated, and easy to understand presentation of forecasts. These challenges were overcome by relying heavily on scientific programming principles [@wilson2014], and using the best open source tools and data formats available regardless of the programing language. In the phenology forecast system described here work still remains in building and implementing state of of the art phenology models, and presenting forecasts in a more user friendly way. Nonetheless this system provides a useable proof of concept for a fully automated, large scale ecological forecast. 

Recent research describing ecological forecast systems discuss only the modelling aspect [@chen2011, @carrillo2018, @doren2018, but see -white2018 for an exception]. Yet the majority of time implementing the system described here was dedicated to data logistics, such as initial data cleaning, and properly building the automated pipeline such that is was robust against failure and informative when it, inevitably, does fail. Here we have described the overall process of implementing a large scale ecological forecast system, reasoning behind key steps, and overall lessons learned, all of which can be useful in other systems. Just as research training today requires more focus on programming skills [@hampton2017], research on complex ecological forecast systems will require descriptions beyond just the modelling component. 

The ability to automatically create continental scale phenology forecasts at a 4km resolution has only recently been possible due to the advent of open source tools and data. Prior to the establishment of the USA-NPN in 2007 there was no large scale phenology observation dataset in the U.S.A. Other long-term phenology datasets are available, but can generally only be used for predictive modelling at the locations they were collected [@taylor2018b]. Efficiently analysing large scale climate and meteorological datasets has only recently been made possible. 
Prior to the integration of dask within xarray in 2015 analysing datasets larger than available memory would have required specialized routines [@hoyer2015], or the use of specialized programming languages (ie. the NCAR Command Language @ncl). In the future as computing power increases, researchers are more commonly trained on advanced scientific coding practices, and more powerful, accessible software tools are made available then ecological forecast systems will hopefully become more common. 

